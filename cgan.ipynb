{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-05T13:24:44.333212Z",
     "iopub.status.busy": "2021-09-05T13:24:44.332846Z",
     "iopub.status.idle": "2021-09-05T13:24:44.340114Z",
     "shell.execute_reply": "2021-09-05T13:24:44.338981Z",
     "shell.execute_reply.started": "2021-09-05T13:24:44.333179Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from fastprogress import progress_bar\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.io import read_image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-05T13:24:44.346173Z",
     "iopub.status.busy": "2021-09-05T13:24:44.345655Z",
     "iopub.status.idle": "2021-09-05T13:24:44.352495Z",
     "shell.execute_reply": "2021-09-05T13:24:44.351449Z",
     "shell.execute_reply.started": "2021-09-05T13:24:44.346131Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config['project_path'] = '/kaggle/input/celeba-dataset'\n",
    "config['img_dir'] = os.path.join(config['project_path'],'img_align_celeba','img_align_celeba')\n",
    "config['working'] = '/kaggle/working'\n",
    "config['lr'] = 1e-3\n",
    "config['batch_size'] = 128\n",
    "config['num_attr'] = 2\n",
    "config['epochs'] = 10\n",
    "config['enc_size'] = 100\n",
    "config['num_workers'] = 8\n",
    "config['img_size'] = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-05T13:24:44.354471Z",
     "iopub.status.busy": "2021-09-05T13:24:44.354051Z",
     "iopub.status.idle": "2021-09-05T13:24:44.366775Z",
     "shell.execute_reply": "2021-09-05T13:24:44.365863Z",
     "shell.execute_reply.started": "2021-09-05T13:24:44.354434Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-05T13:24:44.370443Z",
     "iopub.status.busy": "2021-09-05T13:24:44.370185Z",
     "iopub.status.idle": "2021-09-05T13:24:44.379781Z",
     "shell.execute_reply": "2021-09-05T13:24:44.378349Z",
     "shell.execute_reply.started": "2021-09-05T13:24:44.370415Z"
    }
   },
   "outputs": [],
   "source": [
    "class celebDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, df, num_attr, transform=None):\n",
    "        \"\"\"\n",
    "        img_dir: is the directory to image folder\n",
    "        df: is the dataframe annotation image and attribute\n",
    "        num_attr: number of attribute to random a list of selection attribute\n",
    "        transform: transform augmentation\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.df = df\n",
    "        self.num_attr = num_attr\n",
    "        \n",
    "        self.transform = transform\n",
    "    \n",
    "        self.img_name = list(self.df['image_id'])\n",
    "                \n",
    "        self.attribute = random.sample(list(self.df.columns)[1:], num_attr)\n",
    "        \n",
    "        print('Selected attributes: ', self.attribute)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_name[idx])\n",
    "        \n",
    "        attr = torch.tensor(self.df[self.attribute].iloc[idx])\n",
    "        attr = torch.relu(attr).float()\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, attr\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Generator and Discriminator models\n",
    "![Screen Shot 2021-09-05 at 14.05.58.png](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/1c6682b0-a715-4363-89a2-693ae3d18a8e/Screen_Shot_2021-09-05_at_14.05.58.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210905%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210905T085415Z&X-Amz-Expires=86400&X-Amz-Signature=794ac9751b5f96d6d278f39418de1e144df5946c5cb516413e932aece881a7b2&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Screen%2520Shot%25202021-09-05%2520at%252014.05.58.png%22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-05T13:24:44.381792Z",
     "iopub.status.busy": "2021-09-05T13:24:44.381287Z",
     "iopub.status.idle": "2021-09-05T13:24:44.395494Z",
     "shell.execute_reply": "2021-09-05T13:24:44.394435Z",
     "shell.execute_reply.started": "2021-09-05T13:24:44.381615Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    in_channels (int) – Number of channels in the input image\n",
    "    out_channels (int) – Number of channels produced by the convolution\n",
    "    kernel_size (int or tuple) – Size of the convolving kernel\n",
    "    stride (int or tuple, optional) – Stride of the convolution. Default: 1\n",
    "    padding (int or tuple, optional) – dilation * (kernel_size - 1) - padding zero-padding \n",
    "        will be added to both sides of each dimension in the input. Default: 0\n",
    "    \"\"\"\n",
    "    def __init__(self, num_attr):\n",
    "        super(Generator, self).__init__()\n",
    "        ## deconvolution 1 of image\n",
    "        self.deconv1_1 = nn.ConvTranspose2d(in_channels = 100, \n",
    "                                            out_channels = 512,\n",
    "                                            kernel_size = 4, \n",
    "                                            stride = 1, \n",
    "                                            padding = 0)\n",
    "        self.deconv1_1_bn = nn.BatchNorm2d(512)\n",
    "        \n",
    "        ## deconvolution 2 of label\n",
    "        ## in_channels is number of labels where it is number of desired attributes\n",
    "        \n",
    "        self.deconv1_2 = nn.ConvTranspose2d(in_channels = num_attr, \n",
    "                                            out_channels = 512,\n",
    "                                            kernel_size = 4, \n",
    "                                            stride = 1, \n",
    "                                            padding = 0)\n",
    "        self.deconv1_2_bn = nn.BatchNorm2d(512)\n",
    "        \n",
    "        ## the size*2 for concat between vector images and vector attributes\n",
    "        self.deconv2 = nn.ConvTranspose2d(  in_channels = 1024, \n",
    "                                            out_channels = 512,\n",
    "                                            kernel_size = 4, \n",
    "                                            stride = 2, \n",
    "                                            padding = 1)\n",
    "        self.deconv2_bn = nn.BatchNorm2d(512)\n",
    "        \n",
    "        \n",
    "        self.deconv3 = nn.ConvTranspose2d(  in_channels = 512, \n",
    "                                            out_channels = 256,\n",
    "                                            kernel_size = 4, \n",
    "                                            stride = 2, \n",
    "                                            padding = 1)\n",
    "        self.deconv3_bn = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.deconv4 = nn.ConvTranspose2d(  in_channels = 256, \n",
    "                                            out_channels = 128,\n",
    "                                            kernel_size = 4, \n",
    "                                            stride = 2, \n",
    "                                            padding = 1)\n",
    "        \n",
    "        self.deconv4_bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "        ## out_channels = 3 is the image RGB\n",
    "        ## size 128 with kernel_size =4 and stride =2 --> generating image 64x64\n",
    "        self.deconv5 = nn.ConvTranspose2d(  in_channels = 128, \n",
    "                                            out_channels = 3,\n",
    "                                            kernel_size = 4, \n",
    "                                            stride = 2, \n",
    "                                            padding = 1)\n",
    "\n",
    "    def forward(self, img, attr):\n",
    "        img = F.leaky_relu(self.deconv1_1_bn(self.deconv1_1(img)), 0.2)\n",
    "\n",
    "        attr = F.leaky_relu(self.deconv1_2_bn(self.deconv1_2(attr)), 0.2)\n",
    "        \n",
    "        img = torch.cat([img, attr], 1)\n",
    "        \n",
    "        img = F.leaky_relu(self.deconv2_bn(self.deconv2(img)), 0.2)\n",
    "        img = F.leaky_relu(self.deconv3_bn(self.deconv3(img)), 0.2)\n",
    "        img = F.leaky_relu(self.deconv4_bn(self.deconv4(img)), 0.2)\n",
    "        img = torch.tanh(self.deconv5(img))\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Untitle](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/e96e7a9d-7e8f-4b4d-a2c5-0be80443dcc1/Screen_Shot_2021-09-05_at_14.05.00.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210905%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210905T094004Z&X-Amz-Expires=86400&X-Amz-Signature=070ba6164e6693ba887115113f20e8b96fa98a57378b6e351da8edc1aa4280bf&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Screen%2520Shot%25202021-09-05%2520at%252014.05.00.png%22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-05T13:24:44.397190Z",
     "iopub.status.busy": "2021-09-05T13:24:44.396759Z",
     "iopub.status.idle": "2021-09-05T13:24:44.410545Z",
     "shell.execute_reply": "2021-09-05T13:24:44.409773Z",
     "shell.execute_reply.started": "2021-09-05T13:24:44.397155Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    in_channels (int) – Number of channels in the input image\n",
    "    out_channels (int) – Number of channels produced by the convolution\n",
    "    kernel_size (int or tuple) – Size of the convolving kernel\n",
    "    stride (int or tuple, optional) – Stride of the convolution. Default: 1\n",
    "    padding (int, tuple or str, optional) – Padding added to all four sides of the input. Default: 0\n",
    "    \"\"\"\n",
    "    def __init__(self, num_attr):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.conv1_1 = nn.Conv2d(in_channels = 3, \n",
    "                                 out_channels = 64, \n",
    "                                 kernel_size = 4, \n",
    "                                 stride = 2, \n",
    "                                 padding = 1)\n",
    "        \n",
    "        self.conv1_2 = nn.Conv2d(in_channels = num_attr, \n",
    "                                 out_channels = 64, \n",
    "                                 kernel_size = 4, \n",
    "                                 stride = 2, \n",
    "                                 padding = 1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = 128, \n",
    "                               out_channels = 256, \n",
    "                               kernel_size = 4, \n",
    "                               stride = 2, \n",
    "                               padding = 1)\n",
    "        \n",
    "        self.conv2_bn = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = 256, \n",
    "                               out_channels = 512, \n",
    "                               kernel_size = 4, \n",
    "                               stride = 2, \n",
    "                               padding = 1)\n",
    "        \n",
    "        self.conv3_bn = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels = 512, \n",
    "                               out_channels = 1024, \n",
    "                               kernel_size = 4, \n",
    "                               stride = 2, \n",
    "                               padding = 1)\n",
    "        \n",
    "        self.conv4_bn = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels = 1024, \n",
    "                               out_channels = 1, \n",
    "                               kernel_size = 4, \n",
    "                               stride = 1, \n",
    "                               padding = 0)\n",
    "\n",
    "    def forward(self, img, attr):\n",
    "        img = F.leaky_relu(self.conv1_1(img), 0.2)\n",
    "        attr = F.leaky_relu(self.conv1_2(attr), 0.2)\n",
    "        \n",
    "        img = torch.cat([img, attr], 1)\n",
    "        \n",
    "        img = F.leaky_relu(self.conv2_bn(self.conv2(img)), 0.2)\n",
    "        img = F.leaky_relu(self.conv3_bn(self.conv3(img)), 0.2)\n",
    "        img = F.leaky_relu(self.conv4_bn(self.conv4(img)), 0.2)\n",
    "        img = torch.sigmoid(self.conv5(img))\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-05T13:24:44.606749Z",
     "iopub.status.busy": "2021-09-05T13:24:44.606297Z",
     "iopub.status.idle": "2021-09-05T13:24:45.323946Z",
     "shell.execute_reply": "2021-09-05T13:24:45.322905Z",
     "shell.execute_reply.started": "2021-09-05T13:24:44.606712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected attributes:  ['Pointy_Nose', 'No_Beard']\n"
     ]
    }
   ],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "                    transforms.Resize((config['img_size'],config['img_size'])),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ColorJitter(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean, std)\n",
    "                    ])\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "model_g = Generator(num_attr = config['num_attr'])\n",
    "model_d = Discriminator(num_attr = config['num_attr'])\n",
    "\n",
    "optimizer_g = optim.Adam(model_g.parameters(), lr=config['lr'])\n",
    "optimizer_d = optim.Adam(model_d.parameters(), lr=config['lr'])\n",
    "\n",
    "df_ = pd.read_csv('../input/celeba-dataset/list_attr_celeba.csv')[:20000]\n",
    "\n",
    "celeb_ds = celebDataset(config['img_dir'] , df_, config['num_attr'], train_transforms)\n",
    "celeb_dl = DataLoader(celeb_ds, config['batch_size'], config['num_workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-05T13:24:45.326280Z",
     "iopub.status.busy": "2021-09-05T13:24:45.325903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='157' class='' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [157/157 03:07<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Discriminator: 0.9109365285617436\n",
      "Loss Generator: 5.913375919791544\n",
      "Current epoch: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='157' class='' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [157/157 03:06<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Discriminator: 0.11114628543516704\n",
      "Loss Generator: 6.647100164632129\n",
      "Current epoch: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='157' class='' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [157/157 03:11<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Discriminator: 0.15624066807976983\n",
      "Loss Generator: 8.11368818875331\n",
      "Current epoch: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='157' class='' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [157/157 03:12<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Discriminator: 0.004982543427961605\n",
      "Loss Generator: 9.348012006966172\n",
      "Current epoch: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='157' class='' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [157/157 03:13<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Discriminator: 0.007591987693657102\n",
      "Loss Generator: 9.996645982098428\n",
      "Current epoch: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='157' class='' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [157/157 03:13<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Discriminator: 0.0004969004221956815\n",
      "Loss Generator: 9.93841126618112\n",
      "Current epoch: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='157' class='' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [157/157 03:13<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Discriminator: 0.00019158709367086878\n",
      "Loss Generator: 11.156585265117087\n",
      "Current epoch: 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='69' class='' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      43.95% [69/157 01:23<01:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(config['epochs']):\n",
    "    \n",
    "    print('Current epoch: {}'.format(str(epoch)))\n",
    "    \n",
    "    model_d = model_d.to(device)\n",
    "    model_g = model_g.to(device)\n",
    "    \n",
    "    dis_losses = []\n",
    "    dis_accs = []\n",
    "    \n",
    "    gen_losses = []\n",
    "\n",
    "    \n",
    "    for imgs, attrs in progress_bar(celeb_dl):\n",
    "        imgs, attrs = imgs.to(device), attrs.to(device)\n",
    "        \n",
    "        y_real = torch.full((imgs.shape[0],), 1, dtype=torch.float, device=device)\n",
    "        y_fake = torch.full((imgs.shape[0],), 0, dtype=torch.float, device=device)\n",
    "        \n",
    "        mask_attr = torch.zeros(imgs.shape[0], \n",
    "                                config['num_attr'], \n",
    "                                config['img_size'], \n",
    "                                config['img_size'], \n",
    "                                device = device)\n",
    "        \n",
    "        ## attrs.shape [batch_size, num_attr, 1, 1]\n",
    "        \n",
    "        g_attrs = attrs.unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "        d_attrs = mask_attr + g_attrs\n",
    "        \n",
    "\n",
    "        ###################################################################################\n",
    "        ########### (1) Update D network: maximize log(D(x)) + log(1 - D(G(z))) ###########\n",
    "        ###################################################################################\n",
    "        \n",
    "        model_d.zero_grad()\n",
    "\n",
    "        ## train with real\n",
    "\n",
    "        y_dis_real = model_d(imgs, d_attrs).squeeze()\n",
    "        real_loss_d = criterion(y_dis_real, y_real)\n",
    "        real_acc = torch.mean(1 - torch.abs(y_dis_real - y_real)).item()\n",
    "\n",
    "        ## train with fake\n",
    "        \n",
    "        init_encode_vector = torch.randn(imgs.shape[0], \n",
    "                            config['enc_size'],\n",
    "                            device = device).unsqueeze(-1).unsqueeze(-1)\n",
    "        y_gen = model_g(init_encode_vector, g_attrs)\n",
    "        y_dis_fake = model_d(y_gen, d_attrs).squeeze()\n",
    "        fake_loss_d = criterion(y_dis_fake, y_fake)\n",
    "        fake_acc = torch.mean(1 - torch.abs(y_dis_fake - y_fake)).item()\n",
    "\n",
    "        ## add losses and backprop\n",
    "        \n",
    "        loss_dis = real_loss_d + fake_loss_d\n",
    "        loss_dis.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        ## recording for metrics\n",
    "        dis_losses.append(loss_dis.item())\n",
    "        dis_accs.append((real_acc, fake_acc))\n",
    "        \n",
    "        ###################################################################################\n",
    "        ################### (2) Update G network: maximize log(D(G(z))) ###################\n",
    "        ###################################################################################\n",
    "        \n",
    "        model_g.zero_grad()\n",
    "        \n",
    "        ## get discriminator predictions on faked images, and take loss between real y\n",
    "\n",
    "        y_gen = model_g(init_encode_vector, g_attrs)\n",
    "        y_dis_fake = model_d(y_gen, d_attrs).squeeze()\n",
    "        fake_loss_d = criterion(y_dis_fake, y_real)\n",
    "\n",
    "        ## backprop and record metric\n",
    "        \n",
    "        fake_loss_d.backward()\n",
    "        optimizer_g.step()\n",
    "        gen_losses.append(fake_loss_d.item())\n",
    "    \n",
    "    print('Loss Discriminator: {}'.format(np.mean(dis_losses)))\n",
    "    print('Loss Generator: {}'.format(np.mean(gen_losses)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
