{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom PIL import Image\nfrom fastprogress import progress_bar\nimport random\nimport numpy as np\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.io import read_image\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-05T13:24:44.332846Z","iopub.execute_input":"2021-09-05T13:24:44.333212Z","iopub.status.idle":"2021-09-05T13:24:44.340114Z","shell.execute_reply.started":"2021-09-05T13:24:44.333179Z","shell.execute_reply":"2021-09-05T13:24:44.338981Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"config = {}\n\nconfig['project_path'] = '/kaggle/input/celeba-dataset'\nconfig['img_dir'] = os.path.join(config['project_path'],'img_align_celeba','img_align_celeba')\nconfig['working'] = '/kaggle/working'\nconfig['lr'] = 1e-3\nconfig['batch_size'] = 128\nconfig['num_attr'] = 2\nconfig['epochs'] = 10\nconfig['enc_size'] = 100\nconfig['num_workers'] = 8\nconfig['img_size'] = 64","metadata":{"execution":{"iopub.status.busy":"2021-09-05T13:24:44.345655Z","iopub.execute_input":"2021-09-05T13:24:44.346173Z","iopub.status.idle":"2021-09-05T13:24:44.352495Z","shell.execute_reply.started":"2021-09-05T13:24:44.346131Z","shell.execute_reply":"2021-09-05T13:24:44.351449Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2021-09-05T13:24:44.354051Z","iopub.execute_input":"2021-09-05T13:24:44.354471Z","iopub.status.idle":"2021-09-05T13:24:44.366775Z","shell.execute_reply.started":"2021-09-05T13:24:44.354434Z","shell.execute_reply":"2021-09-05T13:24:44.365863Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Create custom Dataset","metadata":{}},{"cell_type":"code","source":"class celebDataset(Dataset):\n    \n    def __init__(self, img_dir, df, num_attr, transform=None):\n        \"\"\"\n        img_dir: is the directory to image folder\n        df: is the dataframe annotation image and attribute\n        num_attr: number of attribute to random a list of selection attribute\n        transform: transform augmentation\n        \"\"\"\n        self.img_dir = img_dir\n        self.df = df\n        self.num_attr = num_attr\n        \n        self.transform = transform\n    \n        self.img_name = list(self.df['image_id'])\n                \n        self.attribute = random.sample(list(self.df.columns)[1:], num_attr)\n        \n        print('Selected attributes: ', self.attribute)\n        \n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_name[idx])\n        \n        attr = torch.tensor(self.df[self.attribute].iloc[idx])\n        attr = torch.relu(attr).float()\n        img = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            img = self.transform(img)\n        \n        return img, attr\n    \n    def __len__(self):\n        return len(self.df)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T13:24:44.370185Z","iopub.execute_input":"2021-09-05T13:24:44.370443Z","iopub.status.idle":"2021-09-05T13:24:44.379781Z","shell.execute_reply.started":"2021-09-05T13:24:44.370415Z","shell.execute_reply":"2021-09-05T13:24:44.378349Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Build Generator and Discriminator models\n![Screen Shot 2021-09-05 at 14.05.58.png](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/1c6682b0-a715-4363-89a2-693ae3d18a8e/Screen_Shot_2021-09-05_at_14.05.58.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210905%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210905T085415Z&X-Amz-Expires=86400&X-Amz-Signature=794ac9751b5f96d6d278f39418de1e144df5946c5cb516413e932aece881a7b2&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Screen%2520Shot%25202021-09-05%2520at%252014.05.58.png%22)","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    \"\"\"\n    in_channels (int) – Number of channels in the input image\n    out_channels (int) – Number of channels produced by the convolution\n    kernel_size (int or tuple) – Size of the convolving kernel\n    stride (int or tuple, optional) – Stride of the convolution. Default: 1\n    padding (int or tuple, optional) – dilation * (kernel_size - 1) - padding zero-padding \n        will be added to both sides of each dimension in the input. Default: 0\n    \"\"\"\n    def __init__(self, num_attr):\n        super(Generator, self).__init__()\n        ## deconvolution 1 of image\n        self.deconv1_1 = nn.ConvTranspose2d(in_channels = 100, \n                                            out_channels = 512,\n                                            kernel_size = 4, \n                                            stride = 1, \n                                            padding = 0)\n        self.deconv1_1_bn = nn.BatchNorm2d(512)\n        \n        ## deconvolution 2 of label\n        ## in_channels is number of labels where it is number of desired attributes\n        \n        self.deconv1_2 = nn.ConvTranspose2d(in_channels = num_attr, \n                                            out_channels = 512,\n                                            kernel_size = 4, \n                                            stride = 1, \n                                            padding = 0)\n        self.deconv1_2_bn = nn.BatchNorm2d(512)\n        \n        ## the size*2 for concat between vector images and vector attributes\n        self.deconv2 = nn.ConvTranspose2d(  in_channels = 1024, \n                                            out_channels = 512,\n                                            kernel_size = 4, \n                                            stride = 2, \n                                            padding = 1)\n        self.deconv2_bn = nn.BatchNorm2d(512)\n        \n        \n        self.deconv3 = nn.ConvTranspose2d(  in_channels = 512, \n                                            out_channels = 256,\n                                            kernel_size = 4, \n                                            stride = 2, \n                                            padding = 1)\n        self.deconv3_bn = nn.BatchNorm2d(256)\n        \n        self.deconv4 = nn.ConvTranspose2d(  in_channels = 256, \n                                            out_channels = 128,\n                                            kernel_size = 4, \n                                            stride = 2, \n                                            padding = 1)\n        \n        self.deconv4_bn = nn.BatchNorm2d(128)\n        \n        ## out_channels = 3 is the image RGB\n        ## size 128 with kernel_size =4 and stride =2 --> generating image 64x64\n        self.deconv5 = nn.ConvTranspose2d(  in_channels = 128, \n                                            out_channels = 3,\n                                            kernel_size = 4, \n                                            stride = 2, \n                                            padding = 1)\n\n    def forward(self, img, attr):\n        img = F.leaky_relu(self.deconv1_1_bn(self.deconv1_1(img)), 0.2)\n\n        attr = F.leaky_relu(self.deconv1_2_bn(self.deconv1_2(attr)), 0.2)\n        \n        img = torch.cat([img, attr], 1)\n        \n        img = F.leaky_relu(self.deconv2_bn(self.deconv2(img)), 0.2)\n        img = F.leaky_relu(self.deconv3_bn(self.deconv3(img)), 0.2)\n        img = F.leaky_relu(self.deconv4_bn(self.deconv4(img)), 0.2)\n        img = torch.tanh(self.deconv5(img))\n        \n        return img","metadata":{"execution":{"iopub.status.busy":"2021-09-05T13:24:44.381287Z","iopub.execute_input":"2021-09-05T13:24:44.381792Z","iopub.status.idle":"2021-09-05T13:24:44.395494Z","shell.execute_reply.started":"2021-09-05T13:24:44.381615Z","shell.execute_reply":"2021-09-05T13:24:44.394435Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"![Untitle](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/e96e7a9d-7e8f-4b4d-a2c5-0be80443dcc1/Screen_Shot_2021-09-05_at_14.05.00.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210905%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210905T094004Z&X-Amz-Expires=86400&X-Amz-Signature=070ba6164e6693ba887115113f20e8b96fa98a57378b6e351da8edc1aa4280bf&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Screen%2520Shot%25202021-09-05%2520at%252014.05.00.png%22)","metadata":{}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    \"\"\"\n    in_channels (int) – Number of channels in the input image\n    out_channels (int) – Number of channels produced by the convolution\n    kernel_size (int or tuple) – Size of the convolving kernel\n    stride (int or tuple, optional) – Stride of the convolution. Default: 1\n    padding (int, tuple or str, optional) – Padding added to all four sides of the input. Default: 0\n    \"\"\"\n    def __init__(self, num_attr):\n        super(Discriminator, self).__init__()\n        \n        self.conv1_1 = nn.Conv2d(in_channels = 3, \n                                 out_channels = 64, \n                                 kernel_size = 4, \n                                 stride = 2, \n                                 padding = 1)\n        \n        self.conv1_2 = nn.Conv2d(in_channels = num_attr, \n                                 out_channels = 64, \n                                 kernel_size = 4, \n                                 stride = 2, \n                                 padding = 1)\n        \n        self.conv2 = nn.Conv2d(in_channels = 128, \n                               out_channels = 256, \n                               kernel_size = 4, \n                               stride = 2, \n                               padding = 1)\n        \n        self.conv2_bn = nn.BatchNorm2d(256)\n        \n        self.conv3 = nn.Conv2d(in_channels = 256, \n                               out_channels = 512, \n                               kernel_size = 4, \n                               stride = 2, \n                               padding = 1)\n        \n        self.conv3_bn = nn.BatchNorm2d(512)\n        \n        self.conv4 = nn.Conv2d(in_channels = 512, \n                               out_channels = 1024, \n                               kernel_size = 4, \n                               stride = 2, \n                               padding = 1)\n        \n        self.conv4_bn = nn.BatchNorm2d(1024)\n        \n        self.conv5 = nn.Conv2d(in_channels = 1024, \n                               out_channels = 1, \n                               kernel_size = 4, \n                               stride = 1, \n                               padding = 0)\n\n    def forward(self, img, attr):\n        img = F.leaky_relu(self.conv1_1(img), 0.2)\n        attr = F.leaky_relu(self.conv1_2(attr), 0.2)\n        \n        img = torch.cat([img, attr], 1)\n        \n        img = F.leaky_relu(self.conv2_bn(self.conv2(img)), 0.2)\n        img = F.leaky_relu(self.conv3_bn(self.conv3(img)), 0.2)\n        img = F.leaky_relu(self.conv4_bn(self.conv4(img)), 0.2)\n        img = torch.sigmoid(self.conv5(img))\n        \n        return img","metadata":{"execution":{"iopub.status.busy":"2021-09-05T13:24:44.396759Z","iopub.execute_input":"2021-09-05T13:24:44.397190Z","iopub.status.idle":"2021-09-05T13:24:44.410545Z","shell.execute_reply.started":"2021-09-05T13:24:44.397155Z","shell.execute_reply":"2021-09-05T13:24:44.409773Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"mean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntrain_transforms = transforms.Compose([\n                    transforms.Resize((config['img_size'],config['img_size'])),\n                    transforms.RandomHorizontalFlip(),\n                    transforms.ColorJitter(),\n                    transforms.ToTensor(),\n                    transforms.Normalize(mean, std)\n                    ])\n\ncriterion = torch.nn.BCELoss()\n\nmodel_g = Generator(num_attr = config['num_attr'])\nmodel_d = Discriminator(num_attr = config['num_attr'])\n\noptimizer_g = optim.Adam(model_g.parameters(), lr=config['lr'])\noptimizer_d = optim.Adam(model_d.parameters(), lr=config['lr'])\n\ndf_ = pd.read_csv('../input/celeba-dataset/list_attr_celeba.csv')[:20000]\n\nceleb_ds = celebDataset(config['img_dir'] , df_, config['num_attr'], train_transforms)\nceleb_dl = DataLoader(celeb_ds, config['batch_size'], config['num_workers'])","metadata":{"execution":{"iopub.status.busy":"2021-09-05T13:24:44.606297Z","iopub.execute_input":"2021-09-05T13:24:44.606749Z","iopub.status.idle":"2021-09-05T13:24:45.323946Z","shell.execute_reply.started":"2021-09-05T13:24:44.606712Z","shell.execute_reply":"2021-09-05T13:24:45.322905Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Selected attributes:  ['Pointy_Nose', 'No_Beard']\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(config['epochs']):\n    \n    print('Current epoch: {}'.format(str(epoch)))\n    \n    model_d = model_d.to(device)\n    model_g = model_g.to(device)\n    \n    dis_losses = []\n    dis_accs = []\n    \n    gen_losses = []\n\n    \n    for imgs, attrs in progress_bar(celeb_dl):\n        imgs, attrs = imgs.to(device), attrs.to(device)\n        \n        y_real = torch.full((imgs.shape[0],), 1, dtype=torch.float, device=device)\n        y_fake = torch.full((imgs.shape[0],), 0, dtype=torch.float, device=device)\n        \n        mask_attr = torch.zeros(imgs.shape[0], \n                                config['num_attr'], \n                                config['img_size'], \n                                config['img_size'], \n                                device = device)\n        \n        ## attrs.shape [batch_size, num_attr, 1, 1]\n        \n        g_attrs = attrs.unsqueeze(-1).unsqueeze(-1).to(device)\n        d_attrs = mask_attr + g_attrs\n        \n\n        ###################################################################################\n        ########### (1) Update D network: maximize log(D(x)) + log(1 - D(G(z))) ###########\n        ###################################################################################\n        \n        model_d.zero_grad()\n\n        ## train with real\n\n        y_dis_real = model_d(imgs, d_attrs).squeeze()\n        real_loss_d = criterion(y_dis_real, y_real)\n        real_acc = torch.mean(1 - torch.abs(y_dis_real - y_real)).item()\n\n        ## train with fake\n        \n        init_encode_vector = torch.randn(imgs.shape[0], \n                            config['enc_size'],\n                            device = device).unsqueeze(-1).unsqueeze(-1)\n        y_gen = model_g(init_encode_vector, g_attrs)\n        y_dis_fake = model_d(y_gen, d_attrs).squeeze()\n        fake_loss_d = criterion(y_dis_fake, y_fake)\n        fake_acc = torch.mean(1 - torch.abs(y_dis_fake - y_fake)).item()\n\n        ## add losses and backprop\n        \n        loss_dis = real_loss_d + fake_loss_d\n        loss_dis.backward()\n        optimizer_d.step()\n\n        ## recording for metrics\n        dis_losses.append(loss_dis.item())\n        dis_accs.append((real_acc, fake_acc))\n        \n        ###################################################################################\n        ################### (2) Update G network: maximize log(D(G(z))) ###################\n        ###################################################################################\n        \n        model_g.zero_grad()\n        \n        ## get discriminator predictions on faked images, and take loss between real y\n\n        y_gen = model_g(init_encode_vector, g_attrs)\n        y_dis_fake = model_d(y_gen, d_attrs).squeeze()\n        fake_loss_d = criterion(y_dis_fake, y_real)\n\n        ## backprop and record metric\n        \n        fake_loss_d.backward()\n        optimizer_g.step()\n        gen_losses.append(fake_loss_d.item())\n    \n    print('Loss Discriminator: {}'.format(np.mean(dis_losses)))\n    print('Loss Generator: {}'.format(np.mean(gen_losses)))","metadata":{"execution":{"iopub.status.busy":"2021-09-05T13:24:45.325903Z","iopub.execute_input":"2021-09-05T13:24:45.326280Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Current epoch: 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='157' class='' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [157/157 03:07<00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Loss Discriminator: 0.9109365285617436\nLoss Generator: 5.913375919791544\nCurrent epoch: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='157' class='' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [157/157 03:06<00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Loss Discriminator: 0.11114628543516704\nLoss Generator: 6.647100164632129\nCurrent epoch: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='157' class='' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [157/157 03:11<00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Loss Discriminator: 0.15624066807976983\nLoss Generator: 8.11368818875331\nCurrent epoch: 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='157' class='' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [157/157 03:12<00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Loss Discriminator: 0.004982543427961605\nLoss Generator: 9.348012006966172\nCurrent epoch: 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='157' class='' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [157/157 03:13<00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Loss Discriminator: 0.007591987693657102\nLoss Generator: 9.996645982098428\nCurrent epoch: 5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='157' class='' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [157/157 03:13<00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Loss Discriminator: 0.0004969004221956815\nLoss Generator: 9.93841126618112\nCurrent epoch: 6\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='157' class='' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [157/157 03:13<00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Loss Discriminator: 0.00019158709367086878\nLoss Generator: 11.156585265117087\nCurrent epoch: 7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='69' class='' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      43.95% [69/157 01:23<01:46]\n    </div>\n    "},"metadata":{}}]}]}