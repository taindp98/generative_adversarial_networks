{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Resolution GAN\n",
    "\n",
    "GANs are a kind of implicit generative model, which means we train a neural net to produce samples. \n",
    "\n",
    "The implicit generative models don’t let us query the probability of an observation.\n",
    "\n",
    "To rephrase this, we simultaneously train two different networks:\n",
    "\n",
    "1. The generator network G, which tries to generate realistic samples.\n",
    "2. The discriminator network D, which is a binary classification network which tries to classify real vs. fake samples. It takes an input x and computes D(x), the probability it assigns to x being real.\n",
    "\n",
    "The two networks are trained competitively: the generator is trying to fool the discriminator, and the discriminator is trying not to be fooled by the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-11T00:04:36.940592Z",
     "iopub.status.busy": "2021-09-11T00:04:36.940005Z",
     "iopub.status.idle": "2021-09-11T00:04:42.573985Z",
     "shell.execute_reply": "2021-09-11T00:04:42.573287Z",
     "shell.execute_reply.started": "2021-09-11T00:04:36.940506Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as FT\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T00:04:42.575894Z",
     "iopub.status.busy": "2021-09-11T00:04:42.575657Z",
     "iopub.status.idle": "2021-09-11T00:04:42.620923Z",
     "shell.execute_reply": "2021-09-11T00:04:42.620254Z",
     "shell.execute_reply.started": "2021-09-11T00:04:42.575863Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T00:04:42.622833Z",
     "iopub.status.busy": "2021-09-11T00:04:42.622220Z",
     "iopub.status.idle": "2021-09-11T00:04:42.635892Z",
     "shell.execute_reply": "2021-09-11T00:04:42.635160Z",
     "shell.execute_reply.started": "2021-09-11T00:04:42.622797Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['img_dir'] = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba'\n",
    "# config['img_dir'] = '../input/celeba-dataset'\n",
    "\n",
    "\n",
    "config['crop_size'] = 128\n",
    "config['scaling_factor'] = 4\n",
    "config['batch_size'] = 32\n",
    "config['num_workers'] = 6\n",
    "config['lr'] = 5e-4\n",
    "config['beta'] = 1e-3 \n",
    "\n",
    "config['epochs'] = 20\n",
    "\n",
    "config['sch_lr'] = 0.6\n",
    "\n",
    "config['step_size_lr'] = config['epochs'] // 2\n",
    "\n",
    "\n",
    "# Generator parameters\n",
    "config['large_kernel_size_g'] = 9  # kernel size of the first and last convolutions which transform the inputs and outputs\n",
    "config['small_kernel_size_g'] = 3  # kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n",
    "config['n_channels_g'] = 64  # number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n",
    "config['n_blocks_g'] = 16  # number of residual blocks\n",
    "# srresnet_checkpoint = \"./checkpoint_srresnet.pth.tar\"  # filepath of the trained SRResNet checkpoint used for initialization\n",
    "\n",
    "# Discriminator parameters\n",
    "config['kernel_size_d'] = 3  # kernel size in all convolutional blocks\n",
    "config['n_channels_d'] = 64  # number of output channels in the first convolutional block, after which it is doubled in every 2nd block thereafter\n",
    "config['n_blocks_d'] = 8  # number of convolutional blocks\n",
    "config['fc_size_d'] = 1024  # size of the first fully connected layer\n",
    "\n",
    "# A truncated VGG19 network, such that its output is the \n",
    "# feature map obtained by the j-th convolution (after activation)\n",
    "# before the i-th maxpooling layer within the VGG19 network.\n",
    "config['conv_layer'] = 4\n",
    "config['pool_layer'] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize & Convert space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T00:04:42.639978Z",
     "iopub.status.busy": "2021-09-11T00:04:42.639692Z",
     "iopub.status.idle": "2021-09-11T00:04:47.750042Z",
     "shell.execute_reply": "2021-09-11T00:04:47.749310Z",
     "shell.execute_reply.started": "2021-09-11T00:04:42.639946Z"
    }
   },
   "outputs": [],
   "source": [
    "rgb_weights = torch.FloatTensor([65.481, 128.553, 24.966]).to(device)\n",
    "imagenet_mean = torch.FloatTensor([0.485, 0.456, 0.406]).unsqueeze(1).unsqueeze(2)\n",
    "imagenet_std = torch.FloatTensor([0.229, 0.224, 0.225]).unsqueeze(1).unsqueeze(2)\n",
    "imagenet_mean_cuda = torch.FloatTensor([0.485, 0.456, 0.406]).to(device).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "imagenet_std_cuda = torch.FloatTensor([0.229, 0.224, 0.225]).to(device).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "def convert_image(img, source, target):\n",
    "    \"\"\"\n",
    "    Convert an image from a source format to a target format.\n",
    "\n",
    "    :param img: image\n",
    "    :param source: source format, one of 'pil' (PIL image), '[0, 1]' or '[-1, 1]' (pixel value ranges)\n",
    "    :param target: target format, one of 'pil' (PIL image), '[0, 255]', '[0, 1]', '[-1, 1]' (pixel value ranges),\n",
    "                   'imagenet-norm' (pixel values standardized by imagenet mean and std.),\n",
    "                   'y-channel' (luminance channel Y in the YCbCr color format, used to calculate PSNR and SSIM)\n",
    "    :return: converted image\n",
    "    \"\"\"\n",
    "    assert source in {'pil', '[0, 1]', '[-1, 1]'}, \"Cannot convert from source format %s!\" % source\n",
    "    assert target in {'pil', '[0, 255]', '[0, 1]', '[-1, 1]', 'imagenet-norm',\n",
    "                      'y-channel'}, \"Cannot convert to target format %s!\" % target\n",
    "\n",
    "    # Convert from source to [0, 1]\n",
    "    if source == 'pil':\n",
    "        img = FT.to_tensor(img)\n",
    "\n",
    "    elif source == '[0, 1]':\n",
    "        pass  # already in [0, 1]\n",
    "\n",
    "    elif source == '[-1, 1]':\n",
    "        img = (img + 1.) / 2.\n",
    "\n",
    "    # Convert from [0, 1] to target\n",
    "    if target == 'pil':\n",
    "        img = FT.to_pil_image(img)\n",
    "\n",
    "    elif target == '[0, 255]':\n",
    "        img = 255. * img\n",
    "\n",
    "    elif target == '[0, 1]':\n",
    "        pass  # already in [0, 1]\n",
    "\n",
    "    elif target == '[-1, 1]':\n",
    "        img = 2. * img - 1.\n",
    "\n",
    "    elif target == 'imagenet-norm':\n",
    "        if img.ndimension() == 3:\n",
    "            img = (img - imagenet_mean) / imagenet_std\n",
    "        elif img.ndimension() == 4:\n",
    "            img = (img - imagenet_mean_cuda) / imagenet_std_cuda\n",
    "\n",
    "    elif target == 'y-channel':\n",
    "        # Based on definitions at https://github.com/xinntao/BasicSR/wiki/Color-conversion-in-SR\n",
    "        # torch.dot() does not work the same way as numpy.dot()\n",
    "        # So, use torch.matmul() to find the dot product between the last dimension of an 4-D tensor and a 1-D tensor\n",
    "        img = torch.matmul(255. * img.permute(0, 2, 3, 1)[:, 4:-4, 4:-4, :], rgb_weights) / 255. + 16.\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Transforms\n",
    "\n",
    "I will use the `Celeb-A Faces dataset <http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>`__ which can\n",
    "be downloaded at the linked site, or in `GoogleDrive <https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg>`\n",
    "\n",
    "The dataset will download as a file named *img_align_celeba.zip*. Once\n",
    "downloaded, create a directory named *celeba* and extract the zip file\n",
    "into that directory. Then, set the *dataroot* input for this notebook to\n",
    "the *celeba* directory you just created. The resulting directory\n",
    "structure should be:\n",
    "\n",
    "    /path/to/celeba\n",
    "       -> img_align_celeba  \n",
    "           -> 188242.jpg\n",
    "           -> 173822.jpg\n",
    "           -> 284702.jpg\n",
    "           -> 537394.jpg\n",
    "              ...\n",
    "\n",
    "This is an important step because we will be using the ImageFolder\n",
    "dataset class, which requires there to be subdirectories in the\n",
    "dataset’s root folder. Now, we can create the dataset, create the\n",
    "dataloader, set the device to run on, and finally visualize some of the\n",
    "training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T00:04:47.751492Z",
     "iopub.status.busy": "2021-09-11T00:04:47.751222Z",
     "iopub.status.idle": "2021-09-11T00:04:47.762258Z",
     "shell.execute_reply": "2021-09-11T00:04:47.761423Z",
     "shell.execute_reply.started": "2021-09-11T00:04:47.751460Z"
    }
   },
   "outputs": [],
   "source": [
    "class imageTransforms(object):\n",
    "    \"\"\"\n",
    "    Image transformation pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, crop_size, scaling_factor, lr_img_type, hr_img_type, is_train = True):\n",
    "        \n",
    "        self.crop_size = crop_size\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.is_train = is_train\n",
    "        self.lr_img_type = lr_img_type\n",
    "        self.hr_img_type = hr_img_type\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        :param img: a PIL source image from which the HR image will be cropped, and then downsampled to create the LR image\n",
    "        :return: LR and HR images in the specified format\n",
    "        \"\"\"\n",
    "\n",
    "        # Crop\n",
    "        if self.is_train:\n",
    "            # Take a random fixed-size crop of the image, which will serve as the high-resolution (HR) image\n",
    "            left = random.randint(1, img.width - self.crop_size)\n",
    "            top = random.randint(1, img.height - self.crop_size)\n",
    "            right = left + self.crop_size\n",
    "            bottom = top + self.crop_size\n",
    "            hr_img = img.crop((left, top, right, bottom))\n",
    "        else:\n",
    "            # Take the largest possible center-crop of it such that its dimensions are perfectly divisible by the scaling factor\n",
    "            x_remainder = img.width % self.scaling_factor\n",
    "            y_remainder = img.height % self.scaling_factor\n",
    "            left = x_remainder // 2\n",
    "            top = y_remainder // 2\n",
    "            right = left + (img.width - x_remainder)\n",
    "            bottom = top + (img.height - y_remainder)\n",
    "            hr_img = img.crop((left, top, right, bottom))\n",
    "\n",
    "        # Downsize this crop to obtain a low-resolution version of it\n",
    "        lr_img = hr_img.resize((int(hr_img.width / self.scaling_factor), int(hr_img.height / self.scaling_factor)),\n",
    "                               Image.BICUBIC)\n",
    "\n",
    "        # Sanity check\n",
    "        assert hr_img.width == lr_img.width * self.scaling_factor and hr_img.height == lr_img.height * self.scaling_factor\n",
    "\n",
    "        # Convert the LR and HR image to the required type\n",
    "        lr_img = convert_image(lr_img, source='pil', target=self.lr_img_type)\n",
    "        hr_img = convert_image(hr_img, source='pil', target=self.hr_img_type)\n",
    "        \n",
    "        return lr_img, hr_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T00:04:47.763778Z",
     "iopub.status.busy": "2021-09-11T00:04:47.763525Z",
     "iopub.status.idle": "2021-09-11T00:04:47.779155Z",
     "shell.execute_reply": "2021-09-11T00:04:47.778535Z",
     "shell.execute_reply.started": "2021-09-11T00:04:47.763745Z"
    }
   },
   "outputs": [],
   "source": [
    "class superResolutionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset to be used by a PyTorch DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, df, crop_size, scaling_factor, lr_img_type, hr_img_type, is_train):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        self.img_dir = img_dir\n",
    "        self.crop_size = crop_size\n",
    "        self.df = df\n",
    "        self.is_train = is_train\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.lr_img_type = lr_img_type\n",
    "        self.hr_img_type = hr_img_type\n",
    "        self.transform = imageTransforms(crop_size=self.crop_size,\n",
    "                                         scaling_factor=self.scaling_factor,\n",
    "                                         lr_img_type = self.lr_img_type,\n",
    "                                         hr_img_type = self.hr_img_type,\n",
    "                                         is_train = self.is_train)\n",
    "        \n",
    "        self.images = list(df['image_id'])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This method is required to be defined for use in the PyTorch DataLoader.\n",
    "\n",
    "        :param i: index to retrieve\n",
    "        :return: the 'i'th pair LR and HR images to be fed into the model\n",
    "        \"\"\"\n",
    "        # Read image\n",
    "        img_path = os.path.join(self.img_dir,self.images[idx])\n",
    "        img = Image.open(img_path, mode='r').convert('RGB')\n",
    "        lr_img, hr_img = self.transform(img)\n",
    "        return lr_img, hr_img\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        This method is required to be defined for use in the PyTorch DataLoader.\n",
    "\n",
    "        :return: size of this data (in number of images)\n",
    "        \"\"\"\n",
    "        return len(self.images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "To train an SRGAN, a high-resolution image is first downsampled into a lower resolution image and input into a generator. The generator then tries to upsample that image into super resolution. The discriminator is used to compare the generated super-resolution image to the original high-resolution image. The GAN loss from the discriminator is then back propagated into both the discriminator and generator as shown above.\n",
    "\n",
    "The generator uses a number of convolution neural networks (CNNs) and ResNets, along with batch-normalization layers, and ParametricReLU for the activation function. These first downsample the image before upsampling it to generate a super-resolution image. Similarly, the discriminator uses a series of CNNs along with dense layers, a Leaky ReLU, and a sigmoid activation to determine if an image is the original high-resolution image, or the super-resolution image output by the generator.\n",
    "\n",
    "![Untitle](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/bee84325-0bc1-4c04-87d0-087bd0d735d1/Screen_Shot_2021-09-06_at_09.55.49.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210911%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210911T025948Z&X-Amz-Expires=86400&X-Amz-Signature=1bf84a0309c7d8b59f4333e39581a257fdec6ce9df9a002f0c6bec3f34424099&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Screen%2520Shot%25202021-09-06%2520at%252009.55.49.png%22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T00:04:47.782551Z",
     "iopub.status.busy": "2021-09-11T00:04:47.782168Z",
     "iopub.status.idle": "2021-09-11T00:04:47.824673Z",
     "shell.execute_reply": "2021-09-11T00:04:47.823995Z",
     "shell.execute_reply.started": "2021-09-11T00:04:47.782523Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional block, comprising convolutional, BN, activation layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, batch_norm=False, activation=None):\n",
    "        \"\"\"\n",
    "        :param in_channels: number of input channels\n",
    "        :param out_channels: number of output channe;s\n",
    "        :param kernel_size: kernel size\n",
    "        :param stride: stride\n",
    "        :param batch_norm: include a BN layer?\n",
    "        :param activation: Type of activation; None if none\n",
    "        \"\"\"\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "\n",
    "        if activation is not None:\n",
    "            activation = activation.lower()\n",
    "            assert activation in {'prelu', 'leakyrelu', 'tanh'}\n",
    "\n",
    "        # A container that will hold the layers in this convolutional block\n",
    "        layers = list()\n",
    "\n",
    "        # A convolutional layer\n",
    "        layers.append(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=kernel_size // 2))\n",
    "\n",
    "        # A batch normalization (BN) layer, if wanted\n",
    "        if batch_norm is True:\n",
    "            layers.append(nn.BatchNorm2d(num_features=out_channels))\n",
    "\n",
    "        # An activation layer, if wanted\n",
    "        if activation == 'prelu':\n",
    "            layers.append(nn.PReLU())\n",
    "        elif activation == 'leakyrelu':\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "        elif activation == 'tanh':\n",
    "            layers.append(nn.Tanh())\n",
    "\n",
    "        # Put together the convolutional block as a sequence of the layers in this container\n",
    "        self.conv_block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param input: input images, a tensor of size (N, in_channels, w, h)\n",
    "        :return: output images, a tensor of size (N, out_channels, w, h)\n",
    "        \"\"\"\n",
    "        output = self.conv_block(input)  # (N, out_channels, w, h)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class SubPixelConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A subpixel convolutional block, comprising convolutional, pixel-shuffle, and PReLU activation layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size=3, n_channels=64, scaling_factor=2):\n",
    "        \"\"\"\n",
    "        :param kernel_size: kernel size of the convolution\n",
    "        :param n_channels: number of input and output channels\n",
    "        :param scaling_factor: factor to scale input images by (along both dimensions)\n",
    "        \"\"\"\n",
    "        super(SubPixelConvolutionalBlock, self).__init__()\n",
    "\n",
    "        # A convolutional layer that increases the number of channels by scaling factor^2, followed by pixel shuffle and PReLU\n",
    "        self.conv = nn.Conv2d(in_channels=n_channels, out_channels=n_channels * (scaling_factor ** 2),\n",
    "                              kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        # These additional channels are shuffled to form additional pixels, upscaling each dimension by the scaling factor\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=scaling_factor)\n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param input: input images, a tensor of size (N, n_channels, w, h)\n",
    "        :return: scaled output images, a tensor of size (N, n_channels, w * scaling factor, h * scaling factor)\n",
    "        \"\"\"\n",
    "        output = self.conv(input)  # (N, n_channels * scaling factor^2, w, h)\n",
    "        output = self.pixel_shuffle(output)  # (N, n_channels, w * scaling factor, h * scaling factor)\n",
    "        output = self.prelu(output)  # (N, n_channels, w * scaling factor, h * scaling factor)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block, comprising two convolutional blocks with a residual connection across them.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size=3, n_channels=64):\n",
    "        \"\"\"\n",
    "        :param kernel_size: kernel size\n",
    "        :param n_channels: number of input and output channels (same because the input must be added to the output)\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # The first convolutional block\n",
    "        self.conv_block1 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n",
    "                                              batch_norm=True, activation='PReLu')\n",
    "\n",
    "        # The second convolutional block\n",
    "        self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n",
    "                                              batch_norm=True, activation=None)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param input: input images, a tensor of size (N, n_channels, w, h)\n",
    "        :return: output images, a tensor of size (N, n_channels, w, h)\n",
    "        \"\"\"\n",
    "        residual = input  # (N, n_channels, w, h)\n",
    "        output = self.conv_block1(input)  # (N, n_channels, w, h)\n",
    "        output = self.conv_block2(output)  # (N, n_channels, w, h)\n",
    "        output = output + residual  # (N, n_channels, w, h)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class SRResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The SRResNet, as defined in the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, large_kernel_size=9, small_kernel_size=3, n_channels=64, n_blocks=16, scaling_factor=4):\n",
    "        \"\"\"\n",
    "        :param large_kernel_size: kernel size of the first and last convolutions which transform the inputs and outputs\n",
    "        :param small_kernel_size: kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n",
    "        :param n_channels: number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n",
    "        :param n_blocks: number of residual blocks\n",
    "        :param scaling_factor: factor to scale input images by (along both dimensions) in the subpixel convolutional block\n",
    "        \"\"\"\n",
    "        super(SRResNet, self).__init__()\n",
    "\n",
    "        # Scaling factor must be 2, 4, or 8\n",
    "        scaling_factor = int(scaling_factor)\n",
    "        assert scaling_factor in {2, 4, 8}, \"The scaling factor must be 2, 4, or 8!\"\n",
    "\n",
    "        # The first convolutional block\n",
    "        self.conv_block1 = ConvolutionalBlock(in_channels=3, out_channels=n_channels, kernel_size=large_kernel_size,\n",
    "                                              batch_norm=False, activation='PReLu')\n",
    "\n",
    "        # A sequence of n_blocks residual blocks, each containing a skip-connection across the block\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(kernel_size=small_kernel_size, n_channels=n_channels) for i in range(n_blocks)])\n",
    "\n",
    "        # Another convolutional block\n",
    "        self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels,\n",
    "                                              kernel_size=small_kernel_size,\n",
    "                                              batch_norm=True, activation=None)\n",
    "\n",
    "        # Upscaling is done by sub-pixel convolution, with each such block upscaling by a factor of 2\n",
    "        n_subpixel_convolution_blocks = int(math.log2(scaling_factor))\n",
    "        self.subpixel_convolutional_blocks = nn.Sequential(\n",
    "            *[SubPixelConvolutionalBlock(kernel_size=small_kernel_size, n_channels=n_channels, scaling_factor=2) for i\n",
    "              in range(n_subpixel_convolution_blocks)])\n",
    "\n",
    "        # The last convolutional block\n",
    "        self.conv_block3 = ConvolutionalBlock(in_channels=n_channels, out_channels=3, kernel_size=large_kernel_size,\n",
    "                                              batch_norm=False, activation='Tanh')\n",
    "\n",
    "    def forward(self, lr_imgs):\n",
    "        \"\"\"\n",
    "        Forward prop.\n",
    "\n",
    "        :param lr_imgs: low-resolution input images, a tensor of size (N, 3, w, h)\n",
    "        :return: super-resolution output images, a tensor of size (N, 3, w * scaling factor, h * scaling factor)\n",
    "        \"\"\"\n",
    "        output = self.conv_block1(lr_imgs)  # (N, 3, w, h)\n",
    "        residual = output  # (N, n_channels, w, h)\n",
    "        output = self.residual_blocks(output)  # (N, n_channels, w, h)\n",
    "        output = self.conv_block2(output)  # (N, n_channels, w, h)\n",
    "        output = output + residual  # (N, n_channels, w, h)\n",
    "        output = self.subpixel_convolutional_blocks(output)  # (N, n_channels, w * scaling factor, h * scaling factor)\n",
    "        sr_imgs = self.conv_block3(output)  # (N, 3, w * scaling factor, h * scaling factor)\n",
    "\n",
    "        return sr_imgs\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    The generator in the SRGAN, as defined in the paper. Architecture identical to the SRResNet.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, large_kernel_size=9, small_kernel_size=3, n_channels=64, n_blocks=16, scaling_factor=4):\n",
    "        \"\"\"\n",
    "        :param large_kernel_size: kernel size of the first and last convolutions which transform the inputs and outputs\n",
    "        :param small_kernel_size: kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n",
    "        :param n_channels: number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n",
    "        :param n_blocks: number of residual blocks\n",
    "        :param scaling_factor: factor to scale input images by (along both dimensions) in the subpixel convolutional block\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # The generator is simply an SRResNet, as above\n",
    "        self.net = SRResNet(large_kernel_size=large_kernel_size, small_kernel_size=small_kernel_size,\n",
    "                            n_channels=n_channels, n_blocks=n_blocks, scaling_factor=scaling_factor)\n",
    "\n",
    "    def initialize_with_srresnet(self, srresnet_checkpoint):\n",
    "        \"\"\"\n",
    "        Initialize with weights from a trained SRResNet.\n",
    "\n",
    "        :param srresnet_checkpoint: checkpoint filepath\n",
    "        \"\"\"\n",
    "        srresnet = torch.load(srresnet_checkpoint)['model']\n",
    "        self.net.load_state_dict(srresnet.state_dict())\n",
    "\n",
    "        print(\"\\nLoaded weights from pre-trained SRResNet.\\n\")\n",
    "\n",
    "    def forward(self, lr_imgs):\n",
    "        \"\"\"\n",
    "        Forward prop.\n",
    "\n",
    "        :param lr_imgs: low-resolution input images, a tensor of size (N, 3, w, h)\n",
    "        :return: super-resolution output images, a tensor of size (N, 3, w * scaling factor, h * scaling factor)\n",
    "        \"\"\"\n",
    "        sr_imgs = self.net(lr_imgs)  # (N, n_channels, w * scaling factor, h * scaling factor)\n",
    "\n",
    "        return sr_imgs\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    The discriminator in the SRGAN, as defined in the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size=3, n_channels=64, n_blocks=8, fc_size=1024):\n",
    "        \"\"\"\n",
    "        :param kernel_size: kernel size in all convolutional blocks\n",
    "        :param n_channels: number of output channels in the first convolutional block, after which it is doubled in every 2nd block thereafter\n",
    "        :param n_blocks: number of convolutional blocks\n",
    "        :param fc_size: size of the first fully connected layer\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        in_channels = 3\n",
    "\n",
    "        # A series of convolutional blocks\n",
    "        # The first, third, fifth (and so on) convolutional blocks increase the number of channels but retain image size\n",
    "        # The second, fourth, sixth (and so on) convolutional blocks retain the same number of channels but halve image size\n",
    "        # The first convolutional block is unique because it does not employ batch normalization\n",
    "        conv_blocks = list()\n",
    "        for i in range(n_blocks):\n",
    "            out_channels = (n_channels if i is 0 else in_channels * 2) if i % 2 is 0 else in_channels\n",
    "            conv_blocks.append(\n",
    "                ConvolutionalBlock(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                   stride=1 if i % 2 is 0 else 2, batch_norm=i is not 0, activation='LeakyReLu'))\n",
    "            in_channels = out_channels\n",
    "        self.conv_blocks = nn.Sequential(*conv_blocks)\n",
    "\n",
    "        # An adaptive pool layer that resizes it to a standard size\n",
    "        # For the default input size of 96 and 8 convolutional blocks, this will have no effect\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "\n",
    "        self.fc1 = nn.Linear(out_channels * 6 * 6, fc_size)\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.fc2 = nn.Linear(1024, 1)\n",
    "\n",
    "        # Don't need a sigmoid layer because the sigmoid operation is performed by PyTorch's nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param imgs: high-resolution or super-resolution images which must be classified as such, a tensor of size (N, 3, w * scaling factor, h * scaling factor)\n",
    "        :return: a score (logit) for whether it is a high-resolution image, a tensor of size (N)\n",
    "        \"\"\"\n",
    "        batch_size = imgs.size(0)\n",
    "        output = self.conv_blocks(imgs)\n",
    "        output = self.adaptive_pool(output)\n",
    "        output = self.fc1(output.view(batch_size, -1))\n",
    "        output = self.leaky_relu(output)\n",
    "        logit = self.fc2(output)\n",
    "\n",
    "        return logit\n",
    "\n",
    "\n",
    "class TruncatedVGG19(nn.Module):\n",
    "    \"\"\"\n",
    "    A truncated VGG19 network, such that its output is the 'feature map obtained by the j-th convolution (after activation)\n",
    "    before the i-th maxpooling layer within the VGG19 network', as defined in the paper.\n",
    "\n",
    "    Used to calculate the MSE loss in this VGG feature-space, i.e. the VGG loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, i, j):\n",
    "        \"\"\"\n",
    "        :param i: the index i in the definition above\n",
    "        :param j: the index j in the definition above\n",
    "        \"\"\"\n",
    "        super(TruncatedVGG19, self).__init__()\n",
    "\n",
    "        # Load the pre-trained VGG19 available in torchvision\n",
    "        vgg19 = torchvision.models.vgg19(pretrained=True)\n",
    "\n",
    "        maxpool_counter = 0\n",
    "        conv_counter = 0\n",
    "        truncate_at = 0\n",
    "        # Iterate through the convolutional section (\"features\") of the VGG19\n",
    "        for layer in vgg19.features.children():\n",
    "            truncate_at += 1\n",
    "\n",
    "            # Count the number of maxpool layers and the convolutional layers after each maxpool\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                conv_counter += 1\n",
    "            if isinstance(layer, nn.MaxPool2d):\n",
    "                maxpool_counter += 1\n",
    "                conv_counter = 0\n",
    "\n",
    "            # Break if we reach the jth convolution after the (i - 1)th maxpool\n",
    "            if maxpool_counter == i - 1 and conv_counter == j:\n",
    "                break\n",
    "\n",
    "        # Check if conditions were satisfied\n",
    "        assert maxpool_counter == i - 1 and conv_counter == j, \"One or both of i=%d and j=%d are not valid choices for the VGG19!\" % (\n",
    "            i, j)\n",
    "\n",
    "        # Truncate to the jth convolution (+ activation) before the ith maxpool layer\n",
    "        self.truncated_vgg19 = nn.Sequential(*list(vgg19.features.children())[:truncate_at + 1])\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        :param input: high-resolution or super-resolution images, a tensor of size (N, 3, w * scaling factor, h * scaling factor)\n",
    "        :return: the specified VGG19 feature map, a tensor of size (N, feature_map_channels, feature_map_w, feature_map_h)\n",
    "        \"\"\"\n",
    "        output = self.truncated_vgg19(input)  # (N, feature_map_channels, feature_map_w, feature_map_h)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Untitle](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/f7012986-89e5-4040-b16d-674a306708ad/Screen_Shot_2021-09-06_at_10.02.47.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210911%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210911T030458Z&X-Amz-Expires=86400&X-Amz-Signature=9152c75150e66109fc74ca36dc1f55cfd41b3ccdca089466f822cfa62e2e5624&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Screen%2520Shot%25202021-09-06%2520at%252010.02.47.png%22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T00:04:47.827127Z",
     "iopub.status.busy": "2021-09-11T00:04:47.826928Z",
     "iopub.status.idle": "2021-09-11T00:04:47.841482Z",
     "shell.execute_reply": "2021-09-11T00:04:47.840861Z",
     "shell.execute_reply.started": "2021-09-11T00:04:47.827104Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one(\n",
    "        train_loader, \n",
    "        generator, \n",
    "        discriminator, \n",
    "        truncated_vgg19, \n",
    "        content_loss_criterion, \n",
    "        adversarial_loss_criterion,\n",
    "        optimizer_g, \n",
    "        optimizer_d,\n",
    "        beta,\n",
    "        device):\n",
    "    \n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "\n",
    "    :param train_loader: train dataloader\n",
    "    :param generator: generator\n",
    "    :param discriminator: discriminator\n",
    "    :param truncated_vgg19: truncated VGG19 network\n",
    "    :param content_loss_criterion: content loss function (Mean Squared-Error loss)\n",
    "    :param adversarial_loss_criterion: adversarial loss function (Binary Cross-Entropy loss)\n",
    "    :param optimizer_g: optimizer for the generator\n",
    "    :param optimizer_d: optimizer for the discriminator\n",
    "    :param epoch: epoch number\n",
    "    \"\"\"\n",
    "    # Set to train mode\n",
    "    \n",
    "    generator.train()\n",
    "    generator = generator.to(device)\n",
    "    discriminator.train() \n",
    "    discriminator = discriminator.to(device)\n",
    "    truncated_vgg19.eval()\n",
    "    truncated_vgg19 = truncated_vgg19.to(device)\n",
    "    \n",
    "    losses_c = []  # content loss\n",
    "    losses_a = []  # adversarial loss in the generator\n",
    "    losses_d = []  # adversarial loss in the discriminator\n",
    "    \n",
    "    for lr_imgs, hr_imgs in progress_bar(train_loader):\n",
    "        \n",
    "        # Move to default device\n",
    "        # (batch_size (N), 3, 24, 24)\n",
    "        lr_imgs = lr_imgs.to(device)  \n",
    "        \n",
    "        # (batch_size (N), 3, 96, 96)\n",
    "        hr_imgs = hr_imgs.to(device)  \n",
    "\n",
    "        ###################################################################################\n",
    "        ####################  Update G network: maximize log(D(G(z))) #####################\n",
    "        ###################################################################################        \n",
    "\n",
    "        # Generate\n",
    "        \n",
    "        # (N, 3, 96, 96)\n",
    "        sr_imgs = generator(lr_imgs)  \n",
    "        \n",
    "        ## norm\n",
    "        sr_imgs = convert_image(sr_imgs, source='[-1, 1]', target='imagenet-norm')  # (N, 3, 96, 96), imagenet-normed\n",
    "        \n",
    "        # Calculate VGG feature maps for the super-resolved (SR) and high resolution (HR) images\n",
    "        sr_imgs_in_vgg_space = truncated_vgg19(sr_imgs)\n",
    "        \n",
    "        hr_imgs_in_vgg_space = truncated_vgg19(hr_imgs).detach()  # detached because they're constant, targets\n",
    "\n",
    "        # Discriminate super-resolved (SR) images\n",
    "        sr_discriminated = discriminator(sr_imgs)  # (N)\n",
    "\n",
    "        # Calculate the Perceptual loss\n",
    "        content_loss = content_loss_criterion(sr_imgs_in_vgg_space, hr_imgs_in_vgg_space)\n",
    "        \n",
    "        adversarial_loss = adversarial_loss_criterion(sr_discriminated, torch.ones_like(sr_discriminated))\n",
    "        \n",
    "        perceptual_loss = content_loss + beta * adversarial_loss\n",
    "\n",
    "        # Back-prop.\n",
    "        optimizer_g.zero_grad()\n",
    "        perceptual_loss.backward()\n",
    "        \n",
    "        # Update generator\n",
    "        optimizer_g.step()\n",
    "\n",
    "        # Keep track of loss\n",
    "        losses_c.append(content_loss.item())\n",
    "        losses_a.append(adversarial_loss.item())\n",
    "\n",
    "        ###################################################################################\n",
    "        #############  Update D network: maximize log(D(x)) + log(1 - D(G(z))) ############\n",
    "        ###################################################################################\n",
    "\n",
    "        # Discriminate super-resolution (SR) and high-resolution (HR) images\n",
    "        hr_discriminated = discriminator(hr_imgs)\n",
    "        sr_discriminated = discriminator(sr_imgs.detach())\n",
    "        # Binary Cross-Entropy loss\n",
    "                \n",
    "        adversarial_loss =  adversarial_loss_criterion(\n",
    "                                sr_discriminated, \n",
    "                                torch.zeros_like(sr_discriminated)) + \\\n",
    "                            adversarial_loss_criterion(\n",
    "                                hr_discriminated, \n",
    "                                torch.ones_like(hr_discriminated))\n",
    "\n",
    "        # Back-prop.\n",
    "        optimizer_d.zero_grad()\n",
    "        adversarial_loss.backward()\n",
    "\n",
    "        # Update discriminator\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Keep track of loss\n",
    "#         losses_d.append((adversarial_loss.item(), hr_imgs.size(0)))\n",
    "        losses_d.append(adversarial_loss.item())\n",
    "\n",
    "    return np.mean(losses_c), np.mean(losses_a), np.mean(losses_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T00:04:47.842895Z",
     "iopub.status.busy": "2021-09-11T00:04:47.842561Z",
     "iopub.status.idle": "2021-09-11T00:04:47.854087Z",
     "shell.execute_reply": "2021-09-11T00:04:47.853341Z",
     "shell.execute_reply.started": "2021-09-11T00:04:47.842844Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_one(test_loader, model, device):\n",
    "    \n",
    "    list_psnr = []\n",
    "    list_ssim = []\n",
    "    \n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    # Batches\n",
    "        for lr_imgs, hr_imgs in progress_bar(test_loader):\n",
    "            # Move to default device\n",
    "            lr_imgs = lr_imgs.to(device)  # (batch_size (1), 3, w / 4, h / 4), imagenet-normed\n",
    "            hr_imgs = hr_imgs.to(device)  # (batch_size (1), 3, w, h), in [-1, 1]\n",
    "\n",
    "            # Forward prop.\n",
    "            sr_imgs = model(lr_imgs)  # (1, 3, w, h), in [-1, 1]\n",
    "\n",
    "            # Calculate PSNR and SSIM\n",
    "\n",
    "            sr_imgs_y = convert_image(sr_imgs, source='[-1, 1]', target='y-channel').squeeze(0)  # (w, h), in y-channel\n",
    "            hr_imgs_y = convert_image(hr_imgs, source='[-1, 1]', target='y-channel').squeeze(0)  # (w, h), in y-channel\n",
    "            psnr = peak_signal_noise_ratio(hr_imgs_y.cpu().numpy(), sr_imgs_y.cpu().numpy(),data_range=255.)\n",
    "            list_psnr.append(psnr)\n",
    "            \n",
    "            ssim = structural_similarity(hr_imgs_y.cpu().numpy(), sr_imgs_y.cpu().numpy(),\n",
    "                                         data_range=255.)\n",
    "            list_ssim.append(ssim)\n",
    "    \n",
    "    return np.mean(list_psnr), np.mean(list_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T00:04:47.857896Z",
     "iopub.status.busy": "2021-09-11T00:04:47.857685Z",
     "iopub.status.idle": "2021-09-11T00:04:48.812062Z",
     "shell.execute_reply": "2021-09-11T00:04:48.811336Z",
     "shell.execute_reply.started": "2021-09-11T00:04:47.857865Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141819, 60780)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.read_csv('../input/celeba-dataset/list_attr_celeba.csv')\n",
    "# df_ = pd.read_csv('./data/list_attr_celeba.csv')\n",
    "# df_ = df_.sample(50000)\n",
    "df_train, df_valid = train_test_split(df_, test_size = 0.3, random_state=0, shuffle=True)\n",
    "len(df_train),len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T00:04:48.813654Z",
     "iopub.status.busy": "2021-09-11T00:04:48.813391Z",
     "iopub.status.idle": "2021-09-11T00:04:48.851406Z",
     "shell.execute_reply": "2021-09-11T00:04:48.850432Z",
     "shell.execute_reply.started": "2021-09-11T00:04:48.813622Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = superResolutionDataset(\n",
    "                                        img_dir = config['img_dir'],\n",
    "                                        df = df_train,\n",
    "                                        crop_size = config['crop_size'],\n",
    "                                        scaling_factor = config['scaling_factor'],\n",
    "                                        lr_img_type='imagenet-norm',\n",
    "                                        hr_img_type='imagenet-norm',\n",
    "                                        is_train = True\n",
    "                                        )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                                        train_dataset, \n",
    "                                        batch_size = config['batch_size'], \n",
    "                                        shuffle=True, \n",
    "                                        num_workers= config['num_workers'],\n",
    "                                        pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T00:04:48.853430Z",
     "iopub.status.busy": "2021-09-11T00:04:48.852805Z",
     "iopub.status.idle": "2021-09-11T00:04:48.878010Z",
     "shell.execute_reply": "2021-09-11T00:04:48.877248Z",
     "shell.execute_reply.started": "2021-09-11T00:04:48.853394Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_dataset = superResolutionDataset(\n",
    "                                        img_dir = config['img_dir'],\n",
    "                                        df = df_valid,\n",
    "                                        crop_size = 0,\n",
    "                                        scaling_factor = config['scaling_factor'],\n",
    "                                        lr_img_type='imagenet-norm',\n",
    "                                        ## output of Generator\n",
    "                                        hr_img_type='[-1, 1]',\n",
    "                                        is_train = False\n",
    "                                        )\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "                                        valid_dataset, \n",
    "                                        batch_size = config['batch_size'], \n",
    "                                        num_workers= config['num_workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T00:04:48.881268Z",
     "iopub.status.busy": "2021-09-11T00:04:48.881058Z",
     "iopub.status.idle": "2021-09-11T00:04:48.917651Z",
     "shell.execute_reply": "2021-09-11T00:04:48.916944Z",
     "shell.execute_reply.started": "2021-09-11T00:04:48.881238Z"
    }
   },
   "outputs": [],
   "source": [
    "train_resnet_ds = superResolutionDataset(\n",
    "                                        img_dir = config['img_dir'],\n",
    "                                        df = df_train,\n",
    "                                        crop_size = config['crop_size'],\n",
    "                                        scaling_factor = config['scaling_factor'],\n",
    "                                        lr_img_type='imagenet-norm',\n",
    "                                        ## output of Generator\n",
    "                                        hr_img_type='[-1, 1]',\n",
    "                                        is_train = True\n",
    "                                        )\n",
    "\n",
    "train_resnet_dl = torch.utils.data.DataLoader(\n",
    "                                        train_resnet_ds, \n",
    "                                        batch_size = config['batch_size'], \n",
    "                                        num_workers= config['num_workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T00:04:48.918998Z",
     "iopub.status.busy": "2021-09-11T00:04:48.918740Z",
     "iopub.status.idle": "2021-09-11T00:04:48.985046Z",
     "shell.execute_reply": "2021-09-11T00:04:48.984382Z",
     "shell.execute_reply.started": "2021-09-11T00:04:48.918966Z"
    }
   },
   "outputs": [],
   "source": [
    "model_resnet = SRResNet(large_kernel_size=config['large_kernel_size_g'],\n",
    "                          small_kernel_size=config['small_kernel_size_g'],\n",
    "                          n_channels=config['n_channels_g'],\n",
    "                          n_blocks=config['n_blocks_g'],\n",
    "                          scaling_factor=config['scaling_factor'])\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.Adam(model_resnet.parameters(), lr=config['lr'])\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T00:04:48.987440Z",
     "iopub.status.busy": "2021-09-11T00:04:48.987222Z",
     "iopub.status.idle": "2021-09-11T00:04:48.997539Z",
     "shell.execute_reply": "2021-09-11T00:04:48.996640Z",
     "shell.execute_reply.started": "2021-09-11T00:04:48.987417Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_resnet(train_loader, model, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param model: model\n",
    "    :param criterion: content loss function (Mean Squared-Error loss)\n",
    "    :param optimizer: optimizer\n",
    "    :param epoch: epoch number\n",
    "    \"\"\"\n",
    "    model.train()  # training mode enables batch normalization\n",
    "    model = model.to(device)\n",
    "    losses = []\n",
    "\n",
    "    # Batches\n",
    "    for lr_imgs, hr_imgs in progress_bar(train_loader):\n",
    "        \n",
    "        lr_imgs = lr_imgs.to(device)  # (batch_size (N), 3, 24, 24), imagenet-normed\n",
    "        hr_imgs = hr_imgs.to(device)  # (batch_size (N), 3, 96, 96), in [-1, 1]\n",
    "\n",
    "        # Forward prop.\n",
    "        sr_imgs = model(lr_imgs)  # (N, 3, 96, 96), in [-1, 1]\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(sr_imgs, hr_imgs)  # scalar\n",
    "\n",
    "        # Backward prop.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model\n",
    "        optimizer.step()\n",
    "\n",
    "        # Keep track of loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T00:04:49.001046Z",
     "iopub.status.busy": "2021-09-11T00:04:49.000846Z",
     "iopub.status.idle": "2021-09-11T00:39:53.156838Z",
     "shell.execute_reply": "2021-09-11T00:39:53.155946Z",
     "shell.execute_reply.started": "2021-09-11T00:04:49.001025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4432' class='' max='4432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [4432/4432 11:57<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Current epoch: 0\n",
      "Loss: 0.00848774575575386\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4432' class='' max='4432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [4432/4432 11:32<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Current epoch: 1\n",
      "Loss: 0.007129187714594285\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4432' class='' max='4432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [4432/4432 11:33<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Current epoch: 2\n",
      "Loss: 0.006172172973161086\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    losses = train_resnet(train_resnet_dl, model_resnet, criterion, optimizer, device)\n",
    "    print('*'*100)\n",
    "    print('Current epoch: {}'.format(str(epoch)))\n",
    "    print('Loss: {}'.format(np.mean(losses)))\n",
    "    \n",
    "    torch.save({'model': model_resnet,\n",
    "            'optimizer': optimizer},\n",
    "           '/kaggle/working/checkpoint_srresnet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T00:39:53.159201Z",
     "iopub.status.busy": "2021-09-11T00:39:53.158708Z",
     "iopub.status.idle": "2021-09-11T00:40:15.101338Z",
     "shell.execute_reply": "2021-09-11T00:40:15.100590Z",
     "shell.execute_reply.started": "2021-09-11T00:39:53.159158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded weights from pre-trained SRResNet.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c596f1796e4f2fb3f1efb145d3dc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = Generator(large_kernel_size=config['large_kernel_size_g'],\n",
    "                      small_kernel_size=config['small_kernel_size_g'],\n",
    "                      n_channels=config['n_channels_g'],\n",
    "                      n_blocks=config['n_blocks_g'],\n",
    "                      scaling_factor=config['scaling_factor'])\n",
    "\n",
    "generator.initialize_with_srresnet(srresnet_checkpoint='/kaggle/working/checkpoint_srresnet.pth')\n",
    "\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=config['lr'])\n",
    "exp_lr_g = lr_scheduler.StepLR(optimizer_g, step_size = config['step_size_lr'], gamma=config['sch_lr'])\n",
    "\n",
    "discriminator = Discriminator(kernel_size=config['kernel_size_d'],\n",
    "                              n_channels=config['n_channels_d'],\n",
    "                              n_blocks=config['n_blocks_d'],\n",
    "                              fc_size=config['fc_size_d'])\n",
    "\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=config['lr'])\n",
    "exp_lr_d = lr_scheduler.StepLR(optimizer_d, step_size = config['step_size_lr'], gamma=config['sch_lr'])\n",
    "\n",
    "truncated_vgg19 = TruncatedVGG19(i=config['pool_layer'], j=config['conv_layer'])\n",
    "\n",
    "content_loss_criterion = nn.MSELoss()\n",
    "adversarial_loss_criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T00:40:15.102874Z",
     "iopub.status.busy": "2021-09-11T00:40:15.102621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Current epoch: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4432' class='' max='4432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [4432/4432 36:41<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1900' class='' max='1900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1900/1900 07:02<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator learning rate 0.0005\n",
      "Discriminator learning rate 0.0005\n",
      "Loss content: 3.855475848806597\n",
      "Loss advesarial: 0.1207564040137596\n",
      "Loss discrim: 0.48631495087138055\n",
      "Psnr: 27.964229898970647\n",
      "Ssim: 0.9756299567277763\n",
      "****************************************************************************************************\n",
      "Current epoch: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4432' class='' max='4432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [4432/4432 36:41<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1900' class='' max='1900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1900/1900 06:55<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator learning rate 0.0005\n",
      "Discriminator learning rate 0.0005\n",
      "Loss content: 5.635760955756702\n",
      "Loss advesarial: 0.11550603518497858\n",
      "Loss discrim: 0.23555265511022414\n",
      "Psnr: 26.344453982473727\n",
      "Ssim: 0.9646239939160385\n",
      "****************************************************************************************************\n",
      "Current epoch: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4432' class='' max='4432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [4432/4432 36:41<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1900' class='' max='1900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1900/1900 06:53<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator learning rate 0.0005\n",
      "Discriminator learning rate 0.0005\n",
      "Loss content: 10.037861385333015\n",
      "Loss advesarial: 0.11303422432594566\n",
      "Loss discrim: 0.09961414108910036\n",
      "Psnr: 26.927301810362888\n",
      "Ssim: 0.9701674889979035\n",
      "****************************************************************************************************\n",
      "Current epoch: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='760' class='' max='4432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      17.15% [760/4432 06:17<30:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(config['epochs']):\n",
    "    \n",
    "    print('*'*100)\n",
    "    print('Current epoch: {}'.format(str(epoch)))\n",
    "    losses_a, losses_c, losses_d = train_one(\n",
    "                                            train_loader, \n",
    "                                            generator, \n",
    "                                            discriminator, \n",
    "                                            truncated_vgg19, \n",
    "                                            content_loss_criterion, \n",
    "                                            adversarial_loss_criterion,\n",
    "                                            optimizer_g, \n",
    "                                            optimizer_d, \n",
    "                                            config['beta'],\n",
    "                                            device)\n",
    "    \n",
    "    psnr, ssim = eval_one(valid_loader, generator, device)\n",
    "    print(\"Generator learning rate {}\".format(optimizer_g.param_groups[0]['lr']))\n",
    "    print(\"Discriminator learning rate {}\".format(optimizer_d.param_groups[0]['lr']))\n",
    "    print('Loss content: {}'.format(losses_c))\n",
    "    print('Loss advesarial: {}'.format(losses_a))\n",
    "    print('Loss discrim: {}'.format(losses_d))\n",
    "    \n",
    "    print('Psnr: {}'.format(psnr))\n",
    "    print('Ssim: {}'.format(ssim))\n",
    "    \n",
    "    exp_lr_g.step()\n",
    "    exp_lr_d.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
